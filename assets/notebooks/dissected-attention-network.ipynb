{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmfK7SOGEQ25"
   },
   "source": [
    "# The Dissected Attention Network.\n",
    "\n",
    "This notebook is based on the Pytorch [\"Translation with a Sequence to Sequence Network and Attention\"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) tutorial by Sean Robertson. Most of the code is directly copied or only slightly modified. Some of the text cells are also copied directly from there.\n",
    "\n",
    "We will build a basic sequence to sequence network with attention for language translation, while dissecting every function and class to learn the ins and outs of the attention mechanism.\n",
    "\n",
    "This notebook is a byproduct of my learning process. I publish it in the hopes that someone may find some of the bits useful. It focuses on exploring seemingly basic segments that I myself didn't feel comfortable with, while it may skip over other important steps. Feel free to send me suggestions of improvement or clarification!\n",
    "\n",
    "This is my first time using PyTorch, coming from a Keras background, so I will comment on some of my thoughts while using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6A8ngvGEDK9"
   },
   "outputs": [],
   "source": [
    "# Lets import the stuff we will need\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efgoE3gOEDLB"
   },
   "source": [
    "## Loading data files\n",
    "\n",
    "We will use french - english sentence pairs, which we can get from [here](https://download.pytorch.org/tutorial/data.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJexhMUiFCSH"
   },
   "outputs": [],
   "source": [
    "# !wget -nc https://download.pytorch.org/tutorial/data.zip\n",
    "# !unzip -n data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaIdBD0PLbBM"
   },
   "source": [
    "This Lang class keeps track of the word to index dictionaries and word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtAEkxeDEDLE"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0s8WRyCAEDLH"
   },
   "source": [
    "Perform `unicode` -> `ascii` and a bit of data cleaning: lowercase, stripping, separating punctuation and remove non-letter-or-punctiation characters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suJQH0pmEDLH"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # Separate punctuation from word by adding a space\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # Remove anything that isnt a letter or punctuation\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LsNMGJeEDLL"
   },
   "source": [
    "The following function will read from the files and create the `Lang` objects. For me the confusing part here was the necessity of the reverse flag. Why not just exchange `lang1` and `lang2` when calling the function? The reason is that the files that we download are all English -> other language, so `lang1` here has to be English, and then you swap it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShU5uSx9EDLM"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1_uyXxcEDLP"
   },
   "source": [
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we'll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation) and we're filtering to sentences that translate to\n",
    "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
    "earlier).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLjcobI8EDLQ"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "zlZs6dEpEDLU",
    "outputId": "c5716c25-a67f-4dff-ea9d-9089d7a03195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4489\n",
      "eng 2925\n",
      "['vous regardez tout le temps la television .', 'you are always watching tv .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets define here some utility functions to create tensors from sentences. Simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHWhHZpHEDLl"
   },
   "outputs": [],
   "source": [
    "# A few utility functions to transform text into tensors\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what they look like. Seems like the `tensorFromSentence` function is already playing with the tensor shape, unsqueezing an extra empty dim at the end. I suppose this is the `embedding` dimension that will be populated by the `Embedding` layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjqAF_bQ4C94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 123],\n",
       "         [ 298],\n",
       "         [ 126],\n",
       "         [ 247],\n",
       "         [ 557],\n",
       "         [1256],\n",
       "         [   5],\n",
       "         [   1]], device='cuda:0'),\n",
       " tensor([[ 77],\n",
       "         [ 78],\n",
       "         [148],\n",
       "         [156],\n",
       "         [693],\n",
       "         [  4],\n",
       "         [  1]], device='cuda:0'),\n",
       " torch.Size([8, 1]),\n",
       " torch.Size([7, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "t = tensorsFromPair(pair)\n",
    "in_t = t[0]\n",
    "out_t = t[1]\n",
    "in_t, out_t, in_t.shape, out_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The model\n",
    "\n",
    "Here comes the untouched model code from the original tutorial, which we will then dissect to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGkTemrLEDLZ"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtTH_HtrEDLd"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Dissection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the encoder code, I came up with two questions that I would like to answer: \n",
    "- Why is the `.view(1,1,-1)` necessary?\n",
    "- Why do we initialize the hidden state with zeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for testing\n",
    "input_size = input_lang.n_words\n",
    "hidden_size = 64\n",
    "\n",
    "# Encoder has just two components\n",
    "embedding = nn.Embedding(input_size, hidden_size)\n",
    "gru = nn.GRU(hidden_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = indexesFromSentence(input_lang, pairs[0][0])\n",
    "idx = torch.tensor(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = embedding(idx)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = idx.view(1,-1)\n",
    "embedded = embedding(idx)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the embedding layer takes in whatever shape we please, and spits out a tensor with an appended `embedding` dimension. If we check the [documentation](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding), indeed it says: _\"Input: `(\\*)`, LongTensor of arbitrary shape containing the indices to extract\"_. Neat!\n",
    "\n",
    "The `GRU` however is stricter. Checking [its doc page](https://pytorch.org/docs/stable/nn.html?highlight=nn%20rnn#torch.nn.RNN), it turns out that PyTorch RNNs want a shape of `(seq, batch, H_in)`, instead of `(batch, seq, H_in)`, which is what I am used to as a Keras user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx shape: torch.Size([4]).\n",
      "embedded shape: torch.Size([4, 1, 64]).\n"
     ]
    }
   ],
   "source": [
    "# A `(seq, batch, H_in) tensor for the GRU\n",
    "idx = indexesFromSentence(input_lang, pairs[0][0])\n",
    "idx = torch.tensor(idx)\n",
    "print(f\"idx shape: {idx.shape}.\")\n",
    "embedded = embedding(idx)\n",
    "embedded = embedded.unsqueeze(1) # Create batch dim\n",
    "print(f\"embedded shape: {embedded.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "text",
    "id": "lpTRRGHeeETN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 64]), torch.Size([1, 1, 64]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out,hidden = gru(embedded)\n",
    "out.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GRU` gives us an output tensor of the same shape as the input (because we gave it the same input and output size during initialization) and its last hidden state, which is why the `seq` dimension is empty there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Now that we understand how the shapes work here, we can answer why we needed the `view(1,1,-1)` in the original script. It is very likely that during training, the encoder receives as input a rank 0 tensor containing only the index of a single word, which is transformed into a rank 1 tensor `(embedding)`, and finally the `view` inflates it into a `(seq, batch, embedding)`, where both `seq` and `batch` are 1. Seems like an interesting challenge at the end of this script could be to improve this to correctly pass tensors with filled `seq`s and `batch`es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also try to answer why we initialize with zeros by doing a little internet search.\n",
    "\n",
    "A user on [stackexchange](https://datascience.stackexchange.com/questions/27225/how-what-to-initialize-the-hidden-states-in-rnn-sequence-to-sequence-models) highlights that it is important to distinguish between model weights, which will be updated through gradient descent, and states, which don't. There seem to be alternatives to zero initialization, but its the most commonly used one, so lets stick with it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Dissection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the decoder! Lets follow the same pattern of initializing everthing and then playing around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "out_size = output_lang.n_words\n",
    "\n",
    "embedding = nn.Embedding(out_size, hidden_size)\n",
    "attn = nn.Linear(hidden_size * 2, MAX_LENGTH)\n",
    "attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
    "dropout = nn.Dropout(0.1)\n",
    "gru = nn.GRU(hidden_size, hidden_size)\n",
    "out_linear = nn.Linear(hidden_size, out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the decoder also works with just one sequence step,\n",
    "# lets take just one element heer\n",
    "idx = indexesFromSentence(input_lang, pairs[0][1][0])\n",
    "idx = torch.tensor(idx)\n",
    "embedded = embedding(idx).unsqueeze(1)\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, we now have an embedded sequence. The idea behind attention is that we will create an additional input to our `GRU`, which is an attention vector, concatenated to target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64])\n",
      "torch.Size([4, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "# The decoder also assumes a context state with empty `seq`\n",
    "# This guy comes from our encoder tests above\n",
    "print(hidden.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next thing that happens is a concatenation\n",
    "concat = torch.cat((embedded[0], hidden[0]), 1)\n",
    "concat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is our embedded token together with the context vector of our encoder, so `(batch, hidden * 2)`. We will run this through a linear which will compress it into `(batch, max_seq)`. This is the core of the attention component.\n",
    "\n",
    "The goal is to produce a vector that says _\"this is how much you should pay attention to each of the outputs of the encoder, during the current decoding step\"_. The information we let it take into account to judge this is the context vector and the current target token. \n",
    "\n",
    "Attention module talking: _\"Looking at this context vector, which tells me more or less what the meaning of the input sentence was, and knowing you are currently at this target token in the prediction sequence, I can tell you that in order to produce the next output token, you better look at **this** particular part of the encoding sequence.\"_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attn(concat)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# We need to turn it into a probability distribution,\n",
    "# so we throw it through a softmax\n",
    "attn_weights = F.softmax(attention, dim=1)\n",
    "print(attn_weights.shape)\n",
    "# We can check it adds up to one now\n",
    "assert torch.abs(torch.sum(attn_weights.squeeze()) - 1) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder expects `encoder_outputs` to have a shape of `(seq, hidden)`, with `seq` of `MAX_LENGTH`, so lets build a fake one here.\n",
    "\n",
    "After having a peek at the training loop, this is another thing that makes this code quite messy: the batch dim is now absent here again, although we artificially created one with the `view(1,1,-1)` at the encoder. The training loop will remove it, and later we will once again add a fake one. Lets stick to what they give us, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.zeros([MAX_LENGTH, hidden_size])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.bmm` is a batch matrix product, which basically means it takes rank 3 tensors and interprets the first dim as a batch, and performs `torch.mm` on the two remaining dims.\n",
    "\n",
    "Again, to me this line of code looks quite silly since we are adding an empty batch dimension to then use the `bmm` function. We could have used `mm` directly, and make it way easier to understand. In a real model where batches are treated with a bit more respect, `bmm` would be necessary though, which is why I assume they kept it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_applied = torch.bmm(attn_weights.unsqueeze(0), out.unsqueeze(0))\n",
    "attn_applied.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we build the input for the GRU\n",
    "gru_input = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "gru_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could technically use this already to feed it to the `gru`, but I suppose sticking a linear layer here helps reduce the size of the recurrent kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_input = F.relu(attn_combine(gru_input).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we have unsqueezed to bring back the batch dim, which we need because `GRU` is grumpy about dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 64]), torch.Size([1, 1, 64]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, hidden = gru(gru_input, hidden)\n",
    "output.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2925])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A final linear layer and we are done\n",
    "output = F.log_softmax(out_linear(output[0]), dim=1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pfew. Lets see how the training loop is done, although now that we understand all the shapes, it shouldn't be much of an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dissection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will directly break things down here because the original functions are quite long. The goal is not to produce beautiful code but to play around with the internals, so lets get our knive ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models and variables\n",
    "hidden_size = 256\n",
    "lr = 0.001\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "# Create encoder and decoder optimizers\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gotta define our loss function. The models already contain a `log_softmax` function, so we have to pick the loss function that wants -log probabilities as input: `NLLLoss`. We could have used the `CrossentropyLoss`, which includes both the `log_softmax` and final loss calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rerun from here** as many times as you want to manually perform training on your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab some data to train with\n",
    "\n",
    "# Grab random data\n",
    "# pair = random.choice(pairs)\n",
    "# input_sentence, output_sentence = pair\n",
    "# input_tensor, target_tensor = tensorsFromPair(pair)\n",
    "\n",
    "# Grab the same pair always to see the loss decrease\n",
    "pair = pairs[88]\n",
    "input_sentence, output_sentence = pair\n",
    "input_tensor, target_tensor = tensorsFromPair(pair)\n",
    "\n",
    "# Create the initial hidden state, shape: (1, 1, hidden_size)\n",
    "encoder_hidden = encoder.initHidden()\n",
    "# Reset the optimizer accumulated gradients\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "# Get seq length\n",
    "input_length = input_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "# This is where we will keep the results of the encoder\n",
    "encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "# Set initial loss\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the encoder manually with a for loop. I think it would have been possible to give the encoder a proper `seq` dimension and let it handle the loop internally, which would be much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the outputs from the encoder\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(\n",
    "        input_tensor[ei], encoder_hidden)\n",
    "    encoder_outputs[ei] = encoder_output[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor that will hold decoder inputs\n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the code they do a mix of teacher forcing and regular training. Teacher forcing means that you use the true target sequence as input for the decoder, using the outputs only to compute the loss.\n",
    "\n",
    "In regular training, the output of the decoder at each timestep is used as input for the next iteration. Both seem to have advantages and disadvantages, so using fifty fifty of both results is an interesting compromise. Lets go with teacher forcing here, though both are very similarly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass over the context \n",
    "decoder_hidden = encoder_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets keep track of the attention weights, for later visualizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also keep track of the decoded words that the decoder predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would need to investigate on how to avoid a for loop for the decoder. That seems to be more difficult since there are a lot of steps to perform between each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di in range(target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_hidden, encoder_outputs)\n",
    "    loss += criterion(decoder_output, target_tensor[di])\n",
    "    decoder_attentions[di] = decoder_attention\n",
    "    decoder_input = target_tensor[di]\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    if topi.item() == EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "    else:\n",
    "        decoded_words.append(output_lang.index2word[topi.item()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is accumulated in the `loss` variable, and we can call the PyTorch `backward` function to perform a calculation of all our gradients. After calling this function, according to PyTorch docs, the graph that was created during the forwards pass is \"freed\", which means we cannot call `backward` a second time. We first have to reset the weights and perform another forward pass.\n",
    "\n",
    "Something interesting that happened here is that, although our `loss` was initialized as an integer (`loss = 0`), as soon as we run the line `loss += criterion(...)`, the `int` will be promoted to a PyTorch tensor, which has the method `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.0643, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a weight update\n",
    "encoder_optimizer.step()\n",
    "decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the cells between the cell marked above as \"rerun from here\" to here, we should see the average loss dropping over time. However, as we are grabbing a random sentence every time, there are big changes in the values each iteration. You can comment out that line and uncomment the one where we take the same value every time. The model will of course overfit on a single sequence, but we will be able to see the loss dropping consistently, which is pleasant to see after dissected the whole thing and made it run \"manually\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra! Visualizing the attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder implemented by the original notebook already returns the attention weights, which we have caught above during training, and can plot with `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = decoder_attentions.detach().cpu().numpy()\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, unless we are using a properly trained model, these will look quite random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d78d10f588>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more pleasant experience, where we label the axis with the actual words that composed the sentence, we can do something like the following.\n",
    "\n",
    "The predicted sentence would of course again be quite nonsensical since we did not train our model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis saoul .\n",
      "i m drunk .\n",
      "['chances', 'runner', 'bear', 'interpreter', 'suddenly']\n"
     ]
    }
   ],
   "source": [
    "# Input, output and predicted sentences\n",
    "print(input_sentence)\n",
    "print(output_sentence)\n",
    "print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up figure with colorbar\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(attention, cmap='bone')\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                   ['<EOS>'], rotation=90)\n",
    "ax.set_yticklabels([''] + decoded_words)\n",
    "\n",
    "# Show label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit88449b17fd0d4da7b4f7bfa6c3d4762b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
